%        File: hw5.tex
%     Created: Wed Nov 16 06:00 PM 2016 C
% Last Change: Wed Nov 16 06:00 PM 2016 C
%

\documentclass[a4paper]{article}

\title{CSci Homework 5 }
\date{11/30/16}
\author{Trevor Steil}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{bbm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{claim}{Claim}
\newtheorem*{problem}{Problem}
%\newtheorem*{lemma}{Lemma}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\supp}[1]{\mathop{\mathrm{supp}}\left(#1\right)}
\newcommand{\lip}[1]{\mathop{\mathrm{Lip}}\left(#1\right)}
\newcommand{\curl}{\mathrm{curl}}
\newcommand{\la}{\left \langle}
\newcommand{\ra}{\right \rangle}
\renewcommand{\vec}[1]{\mathbf{#1}}

\newenvironment{solution}[1][]{\emph{Solution #1}}

\algnewcommand{\Or}{\textbf{ or }}
\algnewcommand{\BAnd}{\textbf{ and }}

\begin{document}
\maketitle
\begin{enumerate}
  \item Exercise 14.3-3

    \begin{problem}
      Describe an efficient algorithm that, given an interval $i$, returns an interval overlapping $i$ that has the minimum low endpoint, or $T.nil$ if
      no such interval exists.
    \end{problem}

    \begin{solution}

      This algorithm is very similar to \texttt{INTERVAL-SEARCH} given in the textbook. The only difference is that \texttt{INTERVAL-SEARCH} stops as
      soon as it finds an interval that overlaps $i$. After an initial overlapping interval, $i'$ is found, there may be an interval $i''$ in the
      left subtree of $i'$ that overlaps $i$ and because $T$ is sorted by low endpoint of intervals, $i''.low \leq i'.low$. Notice that any interval
      $i_r$ in the right subtree of $i'$ will satisfy $i'.low \leq i_r.low$, so we do not want to continue searching for overlaps in the right subtree
      after an overlap is found.

      Therefore, our algorithm will be the same as \texttt{INTERVAL-SEARCH} except that it will allow for the search to continue along the left
      subtree after an initial overlap is found. This algorithm is given by the following pseudocode:

      \begin{algorithmic}[1]

        \Function{Interval-Search-Min}{$T,i$}

        \State $x \gets T.root$

        \While{ ${x \neq T.nil \BAnd ( i \text{ does not overlap } x.int \Or x.left.max \geq i.low ) }$}
        \If{ $x.left \neq T.nil \BAnd x.left.max \geq i.low$ }
        \State $x \gets x.left$
        \Else
        \State $x \gets x.right$
        \EndIf
        \EndWhile

        \Return x

        \EndFunction

      \end{algorithmic}

      Just as in the case of \texttt{INTERVAL-SEARCH}, this algorithm will return an interval that overlaps $i$ if one exists, or it will return
      $T.nil$ if no such interval exists. By the argument given above, this algorithm will return the interval that overlaps $i$ with the minimum low
      endpoint because at each step in the \texttt{while} loop, we continue searching if $i$ does not overlap the current interval or if there is an
      interval in the left subtree that intersects $i$.

      \texttt{INTERVAL-SEARCH-MIN} runs in $O( \log n )$ time. The underlying data structure for $T$ is a red-black tree, so the height of $T$ is $O(
      \log n)$. \texttt{INTERVAL-SEARCH-MIN} descends one level in $T$ during each interation of the \texttt{while} loop and performs $O(1)$
      operations, giving the claimed running time.

    \end{solution}

  \item

    \begin{problem}
      Let $S$ be a set of $n$ line segments in the plane, where each segment is either horizontal or vertical and is specified by the coordinates of
      its two endpoints. (Assume, for convenience, that no two endpoints in the input have the same $x$- or $y$-coordinate.) Give an $O(n \log
      n)$-time sweepline algorithm, to count the number of pairs of horizontal-vertical segments that intersect.

      The output of your algorithm should merely be an integer equal to the number of intersecting horizontal-vertical pairs. (We are not interested
      here in knowing which pairs of segments intersect, just the number of such pairs.) Your algorithm should use an order-statistics tree (OS-tree)
      as the underlying data structure.

      Describe the main ideas behind your solution (from which correctness should be evident), give pseudocode, and analyze the running time. You
      should use the OS-tree, without modification, as a black-box, i.e., you do not have to write code for the operations you do on this structure.

    \end{problem}

    \begin{solution}

      We can find the number of overlapping intervals by using a vertical sweepline, $D$. We will then keep the heights of ``active'' horizontal intervals
      in an Order-Statistic Tree, $T$. There are three important types of values our sweepline can reach:
      \begin{enumerate}
        \item When $D$ reaches the left endpoint of a horizontal interval, we add the height of the horizontal interval to $T$.
        \item When $D$ reaches the right endpoint of a horizontal interval, we remove the height of the horizontal interval from $T$. (By assumption,
          we have all heights being distinct, so this operation is well-defined.)
        \item When $D$ reaches a vertical segment, we determine how many horizontal intervals this segment intersects. This can be done by temporarily
          adding the lower and upper height values to $T$, using the rank of each to compute the number of intersecting intervals, and then removing
          lower and upper heights for the vertical segment from $T$.
      \end{enumerate}

      This algorithm will require sorting all relevant $x$-values, which takes $O( n \log n )$ time, and inserting and removing nodes from an OS-tree
      which takes $O( \log n )$ time for each insertion and deletion. This will then leave us with an algorithm that runs in $O( n \log n)$ time.

      We will use $(x_i, y_i)$ to denote the enpoints of all intervals in $S$. We have the following pseudocode for this algorithm:
      \begin{algorithmic}[1]

        \Function{CountIntersectingSegments}{$S$}

        \State $T \gets \emptyset$
        \State $intersection \gets 0$
        \State Sort the $(x_i, y_i)$ based on nondecreasing $x$-values. Break any ties (which only happen for the two endpoints of vertical segments,
        by assumption) in order of nondecreasing $y$-values. Use $(x_i, y_i)$ to refer to this list of points in sorted order. Also, note whether each
        $(x_i, y_i)$ belongs to a vertical or horizontal segment.
        \For{ $i \gets 0..2n$}
        \If{ $(x_i,y_i)$ is part of horizontal segment }
        \State $node \gets \texttt{Search}(T,y_i)$
        \If{ $node = T.nil$ } \Comment{ $y_i$ not already in $T$ }
        \State $\texttt{Insert}(T,y_i)$
        \Else \Comment{ $y_i$ already in $T$ }
        \State $\texttt{Delete}(T,y_i)$
        \EndIf
        \ElsIf{ $(x_i,y_i)$ is part of a vertical segment }
        \State $node \gets \texttt{Search}(T,y_i)$
        \If{ $node = T.nil$ }
        \State $\texttt{Insert}(T,y_i)$
        \State $\texttt{Insert}(T,y_{i+1})$
        \State $upper \gets \texttt{Rank}(T,y_{i+1})$
        \State $lower \gets \texttt{Rank}(T,y_i)$
        \State $intersection \gets intersection + upper - lower - 1$
        \Else
        \State $\texttt{Delete}(T,y_i)$
        \State $\texttt{Delete}(T,y_{i-1})$
        \EndIf
        \EndIf
        \EndFor

        \Return $intersection$

        \EndFunction

      \end{algorithmic}

      In this algorithm, we use the \texttt{Search} operation to determine if endpoints of horizontal segments are left or right endpoints based on
      whether or not the associated height is already in the OS-tree (right endpoints will already have their heights entered, left endpoints will
      not). Similarly, we use \texttt{Search} to determine if endpoints of vertical segments are top or bottom endpoints. In order for this to work,
      we require that distinct intervals must have unique heights for its endpoints, which is an assumption given in the problem.

      The algorithm given begins by sorting the list of $2n$ endpoints, which takes $O(n \log n)$ time. In each iteration of the \texttt{for} loop, a
      single \texttt{Search} is performed along with one or two \texttt{Insert} or \texttt{Delete}, all of which take $O(\log n)$ time. In the case of
      a new vertical segment being reached, we add two \texttt{Rank} operations, which also require $O(\log n)$ time. This loop is executed $2n$
      times, so the overall running time, including the sort, is $O(n \log n)$.

    \end{solution}

  \item Problem 17-2

    \begin{problem}
      Binary search of a sorted array takes logarithmic search time, but the time to insert a new element is linear in the size of the array. We can
      improve the time for insertion by keeping several sorted arrays.

      Specifically, suppose that we wish to support \texttt{SEARCH} and \texttt{INSERT} on a set of $n$ elements. Let $k = \lceil \log(n+1) \rceil$,
      and let the binary representation of $n$ be $\la n_{k-1}, n_{k-2}, \dots, n_0 \ra$. We have $k$ sorted arrays $A_0, A_1, \dots, A_{k-1}$, where
      for $i = 0,1,\dots, k-1$, the length of array $A_i$ is $2^i$. Each array is either full or empty, depending on whether $n_i = 1$ or $n_i = 0$,
      respectively. The total number of elements held in all $k$ arrays is therefore $\sum_{i=0}^{k-1} n_i 2^i = n$. Although each individual array is
      sorted, elements in different arrays bear no particular relationship to each other.

      \begin{enumerate}
        \item Describe how to perform the \texttt{SEARCH} operation for this data structure. analyze its worst-case running time.

        \item Describe how to perform the \texttt{INSERT} operation. analyze its worst-case and amortized running times.
      \end{enumerate}

      It is enough to describe the search and insertion algorithms in words. In part (b), to analyze the amortized cost for insertion assume that you
      start with an empty structure and do $n$ insertions into it.

      Use the accounting (i.e. credits) method for your analysis. State clearly the invariant that you use and the number of credits assigned to each
      operation.

    \end{problem}

    \begin{solution}

      \begin{enumerate}
        \item
          The \texttt{SEARCH} operation can be performed by searching for the desired element in each of the full arrays. The worst case for this
          searching is when all of the arrays are full and each array must be searched. In this case, a search must be performed on an array of size
          $2^i$ for every $0 \leq i \leq k-1$. Let $T$ be the worst case running time for the \texttt{SEARCH} operation. Because eachi $A_i$ is
          sorted, we can perform a binary search on $A_i$, which has a logarithmic running time. Therefore, we get
          \begin{align*}
            T &= \sum_{i=0}^{k-1} \log( 2^i ) \\
            &= \sum_{i=0}^{k-1} \\
            &= \frac{k(k-1)}{2} \\
            &= O(k^2) \\
            &= O( \log^2 n ) \quad \parbox{5cm}{by definition of $k$}
          \end{align*}

        \item
          To insert an element, we can begin by creating an array $A_0'$ containing only the new element and trying to copy $A_0'$ into $A_0$.
          If $A_0$ is already full, we can sort and merge $A_0$ and $A_0'$ into an array $A_1'$ and try
          copying $A_1'$ into $A_1$. If $A_1$ is already full, we can merge and sort the arrays into a single array, $A_2'$, of size 4
          and try placing this array in $A_2$. We continue merging arrays until we come to an empty array to place the elements in. Notice that
          because each of the $A_i$ is already sorted, merging two arrays in this algorithm is the same as performing the ``Merge'' operation from
          MergeSort, which can be done in linear time. We will only be using a single temporary array $A_i'$ for this algorithm at any given time.

          In the worst case, the $A_i$ is already full for $0 \leq i \leq k-2$, and all elements must be merged into $A_{k-1}$. Let $T$ be the
          worst-case running time of the \texttt{INSERT} operation. We know the merge operation is $O(m)$, where $m$ is the size of the arrays to
          merge. For simplicity of constants involved, we will assume the time to merge two arrays of size $m$ is bounded by $2m$. Then we have
          \begin{align*}
            T &\leq \sum_{i=0}^{k-2} 2 * 2^{i} \\
            &= 2 (2^{k-1} - 2) \\
            &= 2^k - 4 \\
            &\leq 2^{\log(n+1)+1} - 4 \\
            &= 2n - 2 \\
            &= O(n)
          \end{align*}
          So this still gives a worst-case running time that is linear in $n$.

          Next, we must perform an amortized analysis. First we notice that in our \texttt{INSERT} operation, we try inserting the element into $A_0$,
          but it may need to be merged with other arrays and moved into $A_i$ for $i>0$, but it can only be merged into subsequent $A_i$ arrays at
          most $k$ times. We will assign each element $2k+2$ credits when it is inserted.

          \underline{Invariant:} Each element in array $A_i$ has at least $2(k-i)$ credits for all $i$.

          This invariant vacuously holds upon initialization because all arrays are empty.

          To prove this invariant holds after insertion of a new element, we will need to simultaneously prove that each element of $A_i'$ has at
          least $2(k-i) + 1$ credits. This claim is clear if no $A_i$ is full.

          Now assume the invariant holds before insertion of another element. If $A_0$ is empty, the new element is inserted into $A_0'$, leaving it
          with $2k+1 = 2(k-0)+1$ credits, so the invariant for $A_0'$ holds. This array is then copied to $A_0$, which requires a single insertion,
          leaving $2k = 2(k-0)$ credits for the single element in $A_0$. All subsequent arrays are untouched, so the invariant holds.

          Now we must handle the case of merges. Assume the
          invariant holds for $A_i$ and $A_i'$.

          If during an insertion, we have $A_i'$ full and $A_i$ empty, we copy $A_i'$ into $A_i$, which requires 1 credit from each element in $A_i'$.
          This leaves each element with $2(k-1)$ credits, and the invariant for $A_i$ is satisfied.

          Now assume, $A_i'$ and $A_i$ are both full. Then the two arrays must be merged, which requires $2i$ credits by assumption, that is, it
          requires 1 credit from each element in $A_i$ and $A_i'$. This creates the array $A_{i+1}'$. Then each element of $A_{i+1}'$ has at least
          \begin{align*}
            2(k-i) - 1 &= 2(k-(i+1)) + 1 \\
          \end{align*}
          credits if it started in $A_i$, and it has at least
          \begin{align*}
            2(k-i) &\geq 2(k-(i+1)) + 1
          \end{align*}
          if it started in $A_i'$. Therefore, the invariant is satisfied for $A_{i+1}'$.

          Therefore, we have that during an insertion, the invariant for $A_i'$ holds at every step until an empty $A_i$ is reached. Then after
          copying, the invariant holds for $A_i$. For all $j < i$, $A_j$ is left empty, and for all $j>i$, $A_j$ is never modified. Thus each element
          of $A_i$ has at least $2(k-i)$ credits for all $i$, and the invariant holds.

          Therefore, we have an amortized cost of $2k = O( \log n )$ by the definition of $k$.
      \end{enumerate}

    \end{solution}

  \item

    \begin{problem}
      Let $A$ be a dynamic set of items, each with a real-valued key. Assume that $A$ is empty initially. We wish to support an arbitrary sequence of
      operations on $A$, consisting of the queue operations \texttt{ENQUEUE} and \texttt{DEQUEUE}, and the operation \texttt{MINIMUM} which returns the
      item in $A$ with the minimum key (the item is not removed from $A$). The goal is to develop a data structure for $A$ so that the amortized running
      times of \texttt{ENQUEUE}, \texttt{DEQUEUE}, and \texttt{MINIMUM} are all $O(1)$. (Assume, for simplicity, that the keys in $A$ are always
      distinct.)

      Explain briefly the key ideas underlying your structure, give pseudocode for the operations, and establish the running times using the accounting
      (i.e., credits) method of amortized analysis. State clearly the invariant that you use and the number of credits assigned to each operation.

      Hint: Consider using a queue for \texttt{ENQUEUE} and \texttt{DEQUEUE} and another structure, holding a suitable subset of the queue, for
      \texttt{MINIMUM}.

      Note: The obvious heap-based solution is too expensive and is not of interest here.

    \end{problem}

    \begin{solution}

      A standard queue can efficiently implement the \texttt{ENQUEUE} and \texttt{DEQUEUE} operations. We must work to implement the \texttt{MINIMUM}
      operation in such a way that it doesn't significantly increase the running time of the other operations.

      From the first in, first out order of \texttt{ENQUEUE} and \texttt{DEQUEUE}, we see that when a value is added to the queue, all values already
      in the queue that are larger than the new value could never be the minimum.

      We can use this observation to keep a list of values that contains the sequence of minimums. This means the first entry in the list will be the
      current minimum value in the queue, the second will be the minimum we would have if \texttt{DEQUEUE} operations were performed until the current
      minimum was removed, and so on. We notice that this list is then an increasing sequence of numbers.

      The \texttt{MINIMUM} operation then returns the first entry in this list. Our \texttt{DEQUEUE} operation removes
      the next element from the queue and checks if that element is the first entry in the list. If so, it removes that entry from the list as well.
      Our \texttt{INSERT} operation adds the new entry to our queue and our list. It must then perform the task of making our list an increasing
      sequence that contains the new value, that is all larger values already in the queue must be removed from consideration for being the minimum.

      Let $Q$ be our queue and $S$ be our list of minimum values. We have the following pseudocode:

      \begin{algorithmic}[1]

        \Function{Minimum}{$Q,S$}
        \State
        \Return $S[1]$
        \EndFunction

      \end{algorithmic}

      \begin{algorithmic}[1]

        \Function{Dequeue}{$Q,S$}
        \State $x \gets \texttt{StandardDequeue}(Q)$
        \If{ $x = S[1]$ }
        \State \texttt{Delete}$(S,S[1])$
        \EndIf
        \EndFunction

      \end{algorithmic}

      \begin{algorithmic}[1]

        \Function{Enqueue}{$Q,S,x$}

        \State \texttt{StandardEnqueue}$(Q,x)$

        \While{ $S[length(S)] > x$ }
        \State \texttt{Delete}$(S, S[length(S)])$
        \EndWhile

        \State \texttt{Insert}$(S,x)$

        \EndFunction

      \end{algorithmic}

      From this pseudocode, we can quickly see that \texttt{MINIMUM} and \texttt{DEQUEUE} are $O(1)$ worst-case operations and should therefore have a $O(1)$
      amortized cost as well. On the other hand, \texttt{ENQUEUE} has $O(n)$ worst-case running time.

      Now we must assign our amortized costs to each of these operations. We assign 1 credit to \texttt{MINIMUM}, 1 credit to \texttt{DEQUEUE}, and 6
      credits for \texttt{ENQUEUE}.

      \underline{Invariant}: Every element in $Q$ has 1 credit stored on it, and every element in $S$ has 2 credits stored on it.

      The invariant holds trivially upon initialization, when $Q$ and $S$ are empty.

      Assume the invariant holds. We must not see what happens under each of these operations.

      If the next operation is a \texttt{MINIMUM}, we use the single credit to access $S[1]$. Thus, the invariant still holds.

      Assume the next operation is a \texttt{DEQUEUE}. There is a single credit on the element that is removed from $Q$ that pays for its removal. We
      use the single allocated credit to access $S[1]$. If necessary, we use one of the credits attached to $S[1]$ to remove it from $S$. We kept a
      non-negative number of credits on all elements, and the invariant still holds because all modified elements were removed.

      Assume the next operation is a \texttt{ENQUEUE}. We use one of the credits to insert the element into $Q$, and attach another credit to this new
      element in $Q$ (for later use in \texttt{DEQUEUE}). We use another credit to insert the element into $S$, and attach two credits to this new
      element in $S$. This leaves a single credit to handle the removal of all elements in $S$ greater than the new element. Let $x$ be the element
      being inserted and $y$ be the last element in $S$. If $x < y$, we pay for the comparison and removal of $y$ using the two credits attached to
      $y$ in $S$. If $x > y$, we pay for the comparison with our last available credit. The only elements of $S$ that had credits taken away were
      removed, and the new element has been given one credit in $Q$ and two credits in $S$. Therefore, the invariant holds.

      Therefore, the invariant always holds, and we were able to maintain it without having a credit deficit on any element. This tells us the amortized
      cost of each of the operations is $O(1)$ because we allocated a constant number of credits for each type of operation.

    \end{solution}

  \item

    \begin{problem}
      This problem assumes familiarity with Ch. 6. (Note that Ch. 6 consider max-heaps, whereas the problem below is for min-heaps; however, the two
      notions are symmetric.)

      Consider an implementation of a binary min-heap as a binary tree. It is known that \texttt{INSERT} and \texttt{EXTRACT-MIN} each take time
      $O(\log n)$ in the worst case, where $n$ is the size of the heap. It is possible to use amortized analysis to derive a more informative bound,
      as requested in part (a) below.

      \begin{enumerate}
        \item Use the potential method to prove the following: If an arbitrary sequence of $n$ operations, consisting of \texttt{INSERT} and
          \texttt{EXTRACT-MIN}, is done on an initially-empty heap, then the amortized cost of \texttt{INSERT} is $O(\log n)$ and that of
          \texttt{EXTRACT-MIN} is $O(1)$. Do not change these operations in any way. Describe your potential function carefully and show that it
          works.

          Hint: Relate your potential function to the depths of the nodes in the heap.

        \item
          Is it possible to achieve an amortized cost of $O(\log n)$ for \texttt{EXTRACT-MIN} and $O(1)$ for \texttt{INSERT}? Justify your answer.

      \end{enumerate}

    \end{problem}

    \begin{solution}

      \begin{enumerate}
        \item
          Let $c_i$ be the actual cost of the $i^{th}$ operation and $\widehat{c}_i$ be the amortized cost. We have the relation
          \[ \widehat{c}_i = c_i + \varphi_i - \varphi_{i-1} \]
          that will be motivating our choice of potential function, $\varphi$. We observe that $\log n$ is the maximum height of the min-heap after $n$
          operations, so at any point in this sequence $h(T) \leq \log n$.

          We know the cost of an \texttt{EXTRACT-MIN} operation is $O(\log n)$
          because it makes use of the \texttt{HEAPIFY}. In order to get an amortized cost of $O(1)$, we must have $\Delta
          \varphi \approx -\log n$.

          We know the cost of an \texttt{INSERT} operation is $O(\log n)$ because it places the new value as a leaf and exchanges it with its parent until
          reaching a point that the min-heap property is restored, which could require tracing all of the way back to the root. To get an amortized cost
          of $O(\log n)$, we will need $\Delta \varphi = O(\log n)$.

          Let $T_i$ be the min-heap after the $i^{th}$ operation. Define
          \[ \varphi_i = \varphi(T_i) = \sum_{x \in T_i} ( \lceil \log n \rceil - h(x) ) .\]

          Then $\varphi_0 = 0$ because we are left with an empty sum. Also, $\varphi_i \geq 0$ for all $i$ because $h(x) \leq \log n$ for all $x \in T_i$.

          First, we will look at \texttt{EXTRACT-MIN}. This operation requires deleting the root of the tree, swapping a leaf node to the root, and then
          using \texttt{HEAPIFY}. Therefore, we have $c_i \leq h(T_i) + 2 \leq \log n + 2$. This operation may change the height of the current heap, but
          we have
          \begin{align*}
            \varphi_i - \varphi_{i-1} &= \sum_{x \in T_i} ( \lceil \log n \rceil - h(x) ) - \sum_{x \in T_{i-1}} ( \lceil \log n \rceil - h(x) ) \\
            &= - \lceil \log n \rceil - h(T_i) + h(T_{i-1}) \quad \text{because the sums differ by a single term} \\
            &\leq - \lceil \log n \rceil + 1
          \end{align*}

          Therefore, we have
          \begin{align*}
            \widehat{c}_i &= c_i + \varphi_i - \varphi_{i-1} \\
            &\leq \lceil \log n \rceil + 2 - \lceil \log n \rceil + 1 \\
            &= 3
          \end{align*}

          Therefore, we have \texttt{EXTRACT-MIN} has an amortized cost of $O(1)$.

          Now we look at \texttt{INSERT}. This operation requires placing a new leaf on the heap and then moving it upward until the min-heap property is
          satisfied again. Therefore, $c_i \leq \lceil \log n \rceil + 1$.

          The height of the tree may or may not increase, but we have
          \begin{align*}
            \varphi_i - \varphi_{i-1} &= \sum_{x \in T_i} ( \lceil \log n \rceil - h(x) ) - \sum_{x \in T_{i-1}} ( \lceil \log n \rceil - h(x) ) \\
            &= \lceil \log n \rceil - h(T_i) + h(T_{i-1}) \\
            &\leq \lceil \log n \rceil
          \end{align*}

          Therefore,
          \begin{align*}
            \widehat{c}_i &\leq 2 \lceil \log n \rceil + 1 \\
            &= O(\log n)
          \end{align*}
          Thus, \texttt{INSERT} has an amortized cost of $O(\log n)$.

        \item
          It is not possible to get an amortized cost of $O(\log n)$ for \texttt{EXTRACT-MIN} and $O(1)$ for \texttt{INSERT}.

          Consider the case of all operations in our sequence being \texttt{INSERT} operations. Then by using the aggregate method, we would be able to build an $n$-node min-heap in
          $O(n)$ time. If each element inserted is smaller than all of the previous elements, every single \texttt{INSERT} would require the new key
          to trace all of the way back to the root of the heap in order to restore the min-heap property. Thus, the running time would need to be $O(n
          \log n)$ in this worst-case scenario.

      \end{enumerate}

    \end{solution}

  \item Exercise 17.4-2

    \begin{problem}
      Show that if $\alpha_{i-1} \geq \frac{1}{2}$, and the $i$th operation on a dynamic table is \texttt{TABLE-DELETE}, then the amortized cost of
      the operation with respect to the potential function (17.6) is bounded above by a constant.
    \end{problem}

    \begin{solution}

      First, we notice that the \texttt{TABLE-DELETE} operation cannot cause a contraction of the table unless the table contains a single entry and
      has a size of two. This case has a constant run-time, so we only need to consider when no contraction occurs. We have two cases to consider:
      $\alpha_i \geq \frac{1}{2}$ and $\alpha_i < \frac{1}{2}$.

      Let $T_i$ be the table after the $i^{th}$ operation. Recall that our potential function is defined as
      \[ \varphi_i =
        \begin{cases}
          2 n_i - s_i &\text{if } \alpha_i \geq \frac{1}{2} \\
          \frac{1}{2}s_i - n_i &\text{if } \alpha_i < \frac{1}{2}
        \end{cases}
      \]
      where $n_i$ is the number of elements in $T_i$ and $s_i$ is the size of $T_i$. We see that because we cannot have a contraction of the table,
      $n_i = n_{i-1} - 1$ and $s_i = s_{i-1}$.

      If $\alpha_i \geq \frac{1}{2}$, then we have
      \begin{align*}
        \widehat{c}_i &= c_i + \varphi_i - \varphi_{i-1} \\
        &= 1 + 2n_i - s_i - 2n_{i-1} + s_{i-1} \\
        &= 1 + 2(n_{i-1} - 1) - s_{i-1} - 2 n_{i-1} + s_{i-1} \\
        &= -1
      \end{align*}

      If $\alpha_i < \frac{1}{2}$, then we must have $\alpha_{i-1} = \frac{1}{2}$ because $s_k$ is even for all $k$. Therefore, $s_{i-1} = 2 n_{i-1}$.
      This gives us
      \begin{align*}
        \widehat{c}_i &= c_i + \varphi_i - \varphi_{i-1} \\
        &= 1 + \frac{1}{2} s_i - n_i - 2 n_{i-1} + s_{i-1} \\
        &= -3 n_{i-1} + \frac{3}{2} s_{i-1} \\
        &= -3 n_{i-1} + 3 n_{i-1} \\
        &= 0
      \end{align*}

      In all cases, we have $\widehat{c}_i$ is bounded above by a constant.

    \end{solution}

\end{enumerate}
\end{document}


