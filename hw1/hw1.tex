%        File: hw1.tex
%     Created: Fri Sep 09 02:00 PM 2016 C
% Last Change: Fri Sep 09 02:00 PM 2016 C
%

\documentclass[a4paper]{article}

\title{CSci 5421 Homework 1 }
\date{9/21/16}
\author{Trevor Steil}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{esint}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{claim}{Claim}
\newtheorem*{problem}{Problem}
%\newtheorem*{lemma}{Lemma}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\supp}[1]{\mathop{\mathrm{supp}}\left(#1\right)}
\newcommand{\lip}[1]{\mathop{\mathrm{Lip}}\left(#1\right)}
\newcommand{\curl}{\mathrm{curl}}
\newcommand{\la}{\left \langle}
\newcommand{\ra}{\right \rangle}
\renewcommand{\vec}[1]{\mathbf{#1}}

\newenvironment{solution}{\emph{Solution.}}

\begin{document}
\maketitle

\begin{enumerate}
  \item In each case below, use the Master Theorem (MT) to provide tight asymptotic bounds for the
    indicated recurrence. (For each case, assume ${T(1) = \Theta(1)}$.)

    \begin{enumerate}
      \item $T(n) = 2 T( \frac{n}{4} ) + n \log n$, if $n > 1$.
      \item $T(n) = 2 T( n^{\frac{1}{4}} ) + 1$, if $n > 1$.
      \item $T(n) = a T( \frac{n}{b} ) + dn$, if $n > 1$, where $ a \geq 1$ and $b > 1$ are integer constants and $d>0$ is a real constant. Recall
        that this is the recurrence solved by the Little Master Theorem seen in class. There we derived the solution by iterating the recurrence from
        first principles; here you should derive it directly from the MT.
    \end{enumerate}

    \begin{solution}
      \begin{enumerate}
        \item $n^{\log_4 2} = n^{1/2}$. and $n \log n = \Omega(n^{1/2 + \varepsilon})$ with $\varepsilon=\frac{1}{4}$, so the $\varepsilon$-condition holds. We know
          \begin{align*}
            \frac{2}{4} n \log \left( \frac{n}{4} \right) &= \frac{n}{2} \log n - \frac{n}{2} \log 4 \\
            &\leq \frac{n}{2} \log n
          \end{align*}

          Therefore, the regularity condition is satisfied with $c = \frac{1}{2}$. Thus, we can apply the third case of the Master Theorem to get ${T(n) = \Theta( n \log n)}$.

        \item
          Let $m = \log n$ and $S(m) = T(2^m)$. Then our recurrence can be reformulated as
          \begin{align*}
            S(m) &= T(2^m) \\
            &= T(n) \\
            &= 2 T(n^{1/4}) + 1 \\
            &= 2 T \left( (2^m)^{1/4} \right) + 1 \\
            &= 2 T \left( 2^{m/4} \right) + 1 \\
            &= 2 S \left( \frac{m}{4} \right) + 1
          \end{align*}

          From here, we can apply the Master Theorem. We have $m^{\log_4 2} = m^{1/2}$ and $1 = O(m^{1/2 - \varepsilon})$ for any $0<\varepsilon<\frac{1}{2}$. So the $\varepsilon$-condition holds, and we have $S(m) = \Theta( m^{1/2} )$ by the first case of the Master Theorem. Therefore, we get
          \begin{align*}
            T(n) &= T(2^m) \\
            &= S(m) \\
            &= \Theta(m^{1/2}) \\
            &= \Theta( (\log n)^{1/2} )
          \end{align*}

        \item
          First, assume $a > b$. Then $\log_b a > 1$. Therefore, we can write $\log_b a = 1 + \varepsilon$ for some $\varepsilon>0$, and $dn = O(
          n^{\log_b a - \varepsilon})$. Then by the first case of the Master Theorem, $T(n) = \Theta( n^{\log_b a} )$.

          Now assume $a = b$. Then $\log_b a = 1$, and $dn = \Theta( n^{\log_b a} )$. By the second case of the Master Theorem, we have $T(n) = \Theta( n \log n )$.

          Finally, assume $a < b$. Then $\log_b a < 1$, so we can write $\log_b a = 1 - \varepsilon$ for some $\varepsilon > 0$. With this, $dn =
          \Omega( n^{\log_b a + \varepsilon} )$, so the $\varepsilon$-condition is satisfied. Also, the regularity condition holds trivially because $a \frac{dn}{b} = \frac{a}{b} dn$ and
          $\frac{a}{b} < 1$. Thus by the third of the Master Theorem, we know $T(n) = \Theta( n )$.

          Combining these results, we get the statement of the Little Master Theorem:
          \[ T(n) =
            \begin{cases}
              \Theta(n^{\log_b a}) \quad &\text{ if } a > b \\
              \Theta(n \log n) \quad &\text{ if } a = b \\
              \Theta(n) \quad &\text{ if } a < b
            \end{cases}
          \]
      \end{enumerate}

    \end{solution}

  \item
    Let $y$ and $z$ be $n$-bit integers, where $n$ is a power of 3. Consider the following divide-and-conquer algorithm to compute the product $yz$.

    Break $y$ into three $\frac{n}{3}$-bit pieces, $a$, $b$, and $c$; thus $y = a 2^{2n/3} + b 2^{n/3} + c$, where the powers of 2 denote appropriate
    bit-shifting. Similarly, break $z$ into pieces $d,e,$ and $f$. Now compute $yz$ recursively as:
    \[ yz = ad \ 2^{4n/3} + (ae+bd) 2^n + (af + be +cd) 2^{2n/3} + (bf+ce)2^{n/3} + cf. \]
    You may ignore the issue of ``carries'' throughout.
    \begin{enumerate}
      \item  What is the running time of this algorithm as a function of $n$? Justify your answer by writing down and analyzing the recurrence.

        With a view towards improving the running time in part (a), consider the following approach, where we first compute certain intermediate
        products $(r_1, \dots, r_6)$ and use these along with additions and bit-shifts to compute $yz$.
        \[ r_1 = ?, r_2 = (a+b)(d+e), r_3 = be, r_4 = ?, r_5 = cf, r_6 = ?, \text{ and } yz=? .\]

      \item Fill in the missing information above for $r_1, r_4$, and $r_6$ and show how to compute $yz$.

      \item What is the running time of this algorithm and how does it compare with the one we designed in class? Justify your answer by writing down
        and analyzing the recurrence.
    \end{enumerate}

    \begin{solution}
        \begin{enumerate}
          \item
            This algorithm has a running time of $\Theta(n^2)$. As it is written, we split our larger problem into 9 problems of size $\frac{n}{3}$, and have divide and conquer steps
            that involve choosing the correct bits to split at and adding results, both of which are $O(n)$ operations. Our recurrence is therefore
            \[ T(n) =
              \begin{cases}
                1 \quad &\text{ if } n = 1 \\
                9 T \left( \frac{n}{3} \right) + dn &\text{ else}
              \end{cases}
            \]

            Using the first case of the Little Master Theorem, we find that $T(n) = \Theta( n^{\log_3 9} ) = \Theta( n^2 )$.

          \item
            We will have the following intermediates:
            \begin{align*}
              r_1 &= (b+c)(e+f) \\
              &= be + bf + ce + cf \\
              r_2 &= (a+b)(d+e) \\
              &= ad + ae + bd + be \\
              r_3 &= be \\
              r_4 &= ad \\
              r_5 &= cf \\
              r_6 &= (a+c)(d+f) \\
              &= ad + af + cd + cf
            \end{align*}

            Then we can express
            \begin{align*}
                ad &= r_4 \\
                ae + bd &= r_2 - r_4 - r_3 \\
                af + be + cd &= r_6 -r_4 - r_5 + r_3 \\
                bf + ce &= r_1 - r_3 - r_5 \\
                cf = r_5
            \end{align*}

            Therefore,
            \begin{align*}
              yz &= ad \ 2^{4n/3} + (ae + bd) 2^n + (af + be + cd) 2^{2n/3} + (bf + ce) 2^{n/3} + cf \\
              &= r_4 \ 2^{4n/3} + (r_2 - r_4 - r_3) 2^n + (r_6 - r_4 - r_5 + r_3) 2^{2n/3} + (r_1 - r_3 - r_5) 2^{n/3} + r_5
            \end{align*}

          \item
            This new algorithm involves dividing our problem into 6 subproblems of size $\frac{n}{3}$ with the same $O(n)$ divide and conquer steps.
            Therefore, we get the recurrence
            \[ T(n) =
              \begin{cases}
                1 \quad &\text{if } n = 1 \\
                6 T \left( \frac{n}{3} \right) + dn \quad &\text{else}
              \end{cases}
            \]

            Using the first case of the Little Master Theorem, we find ${T(n) = \Theta( n^{\log_3 6} ) = \Theta( n^{1.6309 \dots} )}$.

            The algorithm we found in class gives a funning time of $\Theta( n^{1.59 \dots})$, so our new algorithm has a slightly worse asymptotic
            running-time.
        \end{enumerate}
    \end{solution}

  \item Prof. M.A. Tricks claims to have discovered a new algorithm for multiplying two $n \times n$ matrices. His algorithm is similar to Strassen's,
    except that it partitions each matrix into submatrices of size $n/8 \times n/8$ and computes the desired product using $k$ recursive matrix
    multiplications and a constant number of matrix additions. (Assume, for simplicity, that $n$ is a power of 8). What is the \emph{largest} value of
    $k$ for which the professor's algorithm beats the running time of Strassen's algorithm? Justify your answer carefully using the Master Theorem.

    \begin{solution}

      For Strassen's algorithm, we get a running time of $\Theta(n^{\log_2 7})$. This is the running time we are aiming to beat. With the proposed
      algorithm, the problem is divided into $k$ subproblems of size $\frac{n}{2}$. If we let $T(n)$ be the time necessary to multiply two $n \times
      n$ matrices together, we get the recurrence
      \[ T(n) =
        \begin{cases}
          1 \quad &\text{if } n = 1 \\
          k T \left( \frac{n}{8} \right) + d n^2 \quad &\text{else}
        \end{cases}
      \]

      First, let's assume that $k < 64$, that is $k \leq 63$. Then $\log_8 k < 2$, so $\log_8 k = 2 - \varepsilon$ for some $\varepsilon > 0$.
      Therefore $dn^2 = \Theta( n^{\log_8 k + \varepsilon} )$ and the $\varepsilon$ condition is satisfied. We also immediately see that $k d \left(
      \frac{n}{8} \right)^2 \leq \frac{63}{64} d n^2$, so the regularity condition is also satisfied. Applying the third case of the Master Theorem
      gives $T(n) = \Theta(n^2)$. Thus the new algorithm is better than Strassen's algorithm for $k < 64$.

      Next, assume $k = 64$. Then $\log_8 64 = 2$ and $d n^2 = \Theta( n^{\log_8 64} )$. By the second case of the Master Theorem, $T(n) = \Theta( n^2
      \log n )$. For $k = 64$ the new algorithm once again gives a better algorithm than Strassen's.

      Finally, assume $k>64$. Then $\log_8 k > 2$. Therefore, $\log_8 k = 2 + \varepsilon$ for some $\varepsilon>0$ and $dn^2 = O( n^{\log_8 k +
      \varepsilon} )$. Applying the first case of the Master Theorem gives $T(n) = \Theta( n^{\log_8 k} )$. We need to compare this to the $\Theta(
      n^{\log_2 7} )$ running time given by Strassen's algorithm. This new algorithm is faster if and only if
      \begin{align*}
        & \log_8 k < \log_2 7 \\
        \Leftrightarrow \ & k < 8^{\log_2 7} \\
        \Leftrightarrow \ & k < 2^{3 \log_2 7} \\
        \Leftrightarrow \ & k < 2^{\log_2 7^3} \\
        \Leftrightarrow \ & k < 7^3 = 343
      \end{align*}

      Combining the three cases, we see the new algorithm outperforms Strassen's if the problem is divided such that $k < 343$.

    \end{solution}

  \item In class, we discussed an $O(n \log n)$-time divide-and-conquer algorithm to find the closest pair among $n$ points in the plane, under the
    Euclidean distance metric. Suppose that we wish to now solve the problem under the Manhattan (or city-block) distance metric, defined as follows:
    For points $p = (x_p, y_p)$ and $q = (x_q, y_q)$, the \emph{Manhattan distance} $d(p,q) = | x_p = x_q| + |y_p - y_q|$. Discuss how to modify the
    above algorithm to solve this problem in $O ( n \log n )$ time.

    It is sufficient to \emph{carefully} describe and justify the changes needed in words; pseudocode is not required. Pay particular attention to
    justifying how many points need to be checked during each step of the ``conquer'' phase. Also analyze the running time briefly.

    \begin{solution}

      Much of the algorithm for the Euclidean metric remains unchanged with the Manhattan metric. We still begin by presorting the points by
      nondecreasing $y$-values and sorting points by nondecreasing $x$-value in a separate copy of the array. If the set of points $S$ has $|S| \leq
      3$, we can directly compute the closest point by calculating the distance between all possible pairs. For any larger sets we find the median
      $x$-value for the points (in constant time) from our array sorted by $x$-values. We then split both arrays of points into subarrays based on
      whether their $x$-value is larger or smaller than this median value. We notice that these subarrays we create are sorted exactly as the
      originals were. We then recursively find the closest pair and minimum distance on the points to the left and to the right of the vertical line
      dividing the points. We then take the minimum of the two distances and the corresponding pair.

      The major difference between the implementation with the different metrics comes in finding the closest pair spanning the dividing line. For the
      same reasons as for the Euclidean metric, we only need to scan points that are within the $2 \delta$ by $\delta$ rectangle, $R$, spanning the dividing
      line as we move this rectangle vertically. Under this new metric, that rectangle may contain more points than before. In the proof of the
      sparseness lemma given in class, we used the fact that the farthest two points within a single $\frac{\delta}{2}$ by $\frac{\delta}{2}$
      rectangle may be apart was $\frac{\delta}{\sqrt{2}} < \delta$. With the Manhattan metric, two points in the same rectangle can be a distance
      $\delta$ apart. Therefore we get the following:

      \begin{lemma}[Sparseness Lemma]
        Under the Manhattan metric, the rectangle $R$ can contain at most 12 points.
      \end{lemma}

      \begin{proof}

        Suppose $R$ contains at least 13 points. Divide $R$ into 12 rectangles of width $\frac{\delta}{3}$ and height $\frac{\delta}{2}$. Notice that
        each of these rectangles is completely contained on one side of the dividing line. By the pigeonhole principle, at least one of these
        rectangles must contain at least 2 points. Within this rectangle,
        \[ d(p,q) \leq \frac{\delta}{3} + \frac{\delta}{2} = \frac{5 \delta}{6} < \delta .\]

        But by construction, any two points on the same side of the dividing line must be at least a distance of $\delta$ apart. We have reached a
        contradiction. Therefore, $R$ must contain no more than 12 points.

      \end{proof}

      By this sparseness lemma, as we scan the for points with a minimal distance that span the dividing line using our rectangle $R$, we only need to
      check at most 11 other points at a time. Thus this portion of the algorithm has a running time of $11n$ at most.

      Let $T(n)$ be the time required to find the closest pair of points in a set $S$ that has been presorted. We have the recurrence
      \[ T(n) =
        \begin{cases}
          O(1) &\text{if } n \leq 3 \\
          2 T \left( \frac{n}{2} \right) + O(n) &\text{else}
        \end{cases}
      \]

      This recurrence is true because at each step we split our set in half using our vertical dividing line, and the time required to check for a
      closest pair across the dividing line is $O(n)$ by the argument given above. Applying the Little Master Theorem to our recurrence gives $T(n) =
      \Theta( n \log n )$. This calculation is done assuming our set of points has been presorted. When we add the time required for this presort, we
      get an overall running time that is $\Theta(n \log n)$ because the sorts can be performed in $\Theta(n \log n)$ time as well.

    \end{solution}

  \item Recall the (worst-case) linear-time divide-and-conquer algorithm for finding the $k$th smallest of $n$ reals, where we used groups of size 5.
    Suppose that we use the same algorithm but with groups of size $g$ for some positive integer constant $g$. Derive the recurrence relation for this
    algorithm as a function of $n$ and $g$. (Consider separately the case where $g$ is even and where it is odd).

    Based on your recurrences, determine the smallest integer $g$ for which the algorithm runs in linear time and justify your answer.

    You may ignore floors and ceilings in your derivation and you do not have to write the algorithm itself.

    \begin{solution}

      In this algorithm, we divide our set of $n$ integers into $\frac{n}{g}$ groups of $g$ elements and directly compute the median of each group
      (requiring $O(n)$ time). Then we take the set of all medians and recursively take the median of that group. Call this median $m^\ast$. We then
      sort our set into values greater than $m^\ast$ and values less than $m^\ast$. Our selection algorithm is then called recursively on one of these
      sets based on the relative position we are searching for. This selection algorithm therefore requires two recursive calls to itself.

      Let $T(n)$ be the worst case number of comparisons performed on an input of size $n$. Whether $g$ is even or odd, we require a number of medians
      to be calculated directly, giving a $O(n)$ term. We then calculate the median of these $\frac{n}{g}$ medians recursively, contributing a $T
      \left( \frac{n}{g} \right)$ term.

      Assume $g$ is odd. After calculating $m^\ast$, we are guaranteed that at least approximately $\frac{n}{2g}$ medians are greater than $m^\ast$, and within
      each of those groups $\frac{g+1}{2}$ values are greater than or equal to the respective median. We get the same result for values less than
      $m^\ast$. Therefore, when sorting values that are less than or greater than $m^\ast$, we are able to discard at least $\approx
      \frac{n (g+1)}{4g}$ values from the set we are going to recursively run our selection algorithm on. In the worst case scenario, we are able to
      discard only these values, leaving $\approx n - \frac{n(g+1)}{4g} = \frac{4ng - n(g+1)}{4g}$ values to use in the recursive call. This
      contributes a $T \left( \frac{4ng - n(g+1)}{4g} \right)$ to the worst case running time of our algorithm.

      This gives us the recurrence
      \[ T(n) =
        \begin{cases}
          O(1) &\text{if } n \leq 5 \\
          T \left( \frac{n}{g} \right) + T \left( \frac{4ng - n(g+1)}{4g} \right) + O(n) &\text{else}
        \end{cases}
      \]

      As in class, we will look for $g$ that makes
      \[ \frac{n}{g} + \frac{4ng - n(g+1)}{4g} < n\]

      This gives
      \begin{align*}
        &4n + 4ng - ng - n < 4ng \\
        &\Leftrightarrow n (3 - g) < 0\\
        &\Leftrightarrow g \geq 5
      \end{align*}

      So we expect our selection algorithm to run in linear time for $g \geq 5$.

    \end{solution}

  \item Let $S$ be a set containing all but one of the integers in the range $[0, n]$, where $n+1$ is a power of 2. Assume that $S$ is implemented as
    a linked list. (The list is not necessarily in sorted order.) One way to find the missing integer efficiently, in $\Theta(n)$ time, is to insert
    the elements of $S$ into an array $I[0:n]$ and then scan $I$. However, this assumes that one can access any integer, $i,$, in $S$ in constant time
    with a single operation. Suppose that one adops a different (more restrictive) computational model, where we are able to access only the $j$th bit
    in the inary representation of integer $i$ in constant time. Give a divide-and-conquer algorithm to compute the missing integer in $\Theta(n)$
    time even under this model.

    Your answer should include $(a)$ a brief description of the main ideas from which the correctness of your algorithm should be evident, $(b)$
    pseudocode, and $(c)$ an analysis of the running time.

  \item Let $S$ be a set of $n \geq 2$ distinct real numbers. Let $\max(S)$ and $\min(S)$ be the largest and smallest numbers in $S$, respectively.
    Define $avg\_gap(S)$ to be $\frac{1}{n-1} (\max(S) - \min(S))$; this is the average distance on the real line between consecutive elements of $S$
    when $S$ is in sorted order. Give a divide-and-conquer algorithm to compute distinct elements $x$ and $y$ in $S$ such that $|x-y| \leq
    avg\_gap(S)$. The running time should be $O(n)$. (Note, that the given set $S$ is not necessarily in sorted order. You cannot afford to sort it
    since this would take $O(n \log N)$ time.)

    Your answer should include $(a)$ a brief description of the main ideas from which the correctness of your algorithm should be evident, $(b)$
    pseudocode, and $(c)$ an analysis of the running time.
\end{enumerate}

\end{document}


